\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{enumitem}

\title{ML Pipeline Overview}
\date{}

\begin{document}

\maketitle

\tableofcontents
\bigskip

\section{Overview}
This project builds an end-to-end machine learning pipeline to evaluate technical blueprints by extracting dimensions, tolerances, and annotations---and then classifying them based on production feasibility. This document provides an overview of the pipeline's architecture and the sequential steps involved, as represented in the accompanying UML diagram (\verb|ml_pipeline.puml|).

\section{Table of Contents}
The sections in this document include:
\begin{itemize}[label={--}]
    \item Overview
    \item Pipeline Workflow Steps
    \begin{itemize}[label={-}]
        \item Data Ingestion and Parsing
        \item Blueprint Extraction
        \item Data Preprocessing
        \item Feature Engineering
        \item Modeling
        \item Deployment and Monitoring
        \item Pipeline Orchestration
    \end{itemize}
    \item Why the Steps Are Organized This Way
    \item Conclusion
\end{itemize}

\section{Overview of the ML Pipeline}
The ML pipeline is designed to process complex blueprint data in a sequential and modular manner. The process converts raw blueprint files into structured data, cleans and normalizes that data, extracts meaningful features, and ultimately utilizes multiple machine learning models to determine whether the specified tolerances are achievable in production. The key modules are:
\begin{itemize}[label={--}]
    \item \textbf{Data Layer:} For file ingestion and parsing.
    \item \textbf{Extraction Module:} Converts blueprint images into structured, domain-specific data.
    \item \textbf{Data Processing and Preprocessing:} Cleans and normalizes the extracted data.
    \item \textbf{Feature Engineering:} Transforms normalized data into feature vectors.
    \item \textbf{Modeling:} Comprises several models:
    \begin{itemize}[label={-}]
        \item \textbf{YOLO Detection Model:} Identifies key regions (bounding boxes) in blueprints.
        \item \textbf{Symbol Classification Model:} Classifies symbols (e.g., arrows, icons) within cropped regions.
        \item \textbf{OCR Model:} Extracts text from cropped regions.
        \item \textbf{Rule-Based Feasibility Model:} Integrates model outputs to perform a feasibility analysis.
    \end{itemize}
    \item \textbf{Deployment and Monitoring:} Packages and deploys the final model and monitors its performance.
    \item \textbf{Pipeline Orchestration:} Manages the sequential flow of tasks across the pipeline.
\end{itemize}

\section{Pipeline Workflow Steps}

\subsection{1. Data Ingestion and Parsing}
\begin{itemize}
    \item \textbf{DataIngestion} \\
    \textbf{Method:} \verb|ingestData(sourcePath: str) → RawData| \\
    \textbf{Function:} Reads raw blueprint files (PDFs, CAD files) from specified sources.
    
    \item \textbf{FileParser (Base Class)} \\
    \textbf{Method:} \verb|parse(filePath: str) → RawData| \\
    \textbf{Function:} Provides an interface for parsing files.
    
    \item \textbf{PDFParser \& CADParser (Inherit from FileParser)} \\
    \textbf{Methods:} \verb|parsePDF(filePath: str) → RawData| and \verb|parseCAD(filePath: str) → RawData| \\
    \textbf{Function:} Handle format-specific parsing logic.
\end{itemize}

\subsection{2. Blueprint Extraction}
\begin{itemize}
    \item \textbf{BlueprintExtractionModel} \\
    \textbf{Method:} \verb|extractBlueprintData(filePath: str) → StructuredData| \\
    \textbf{Function:} Uses computer vision techniques (with OCR assistance) to extract dimensions, tolerances, and annotations from blueprints.
    
    \item \textbf{OCRProcessor (Support Class)} \\
    \textbf{Method:} \verb|processImage(imagePath: str) → TextData| \\
    \textbf{Function:} Provides basic OCR capabilities to assist in text extraction during blueprint extraction.
\end{itemize}

\subsection{3. Data Preprocessing}
\begin{itemize}
    \item \textbf{DataPreprocessing} \\
    \textbf{Method:} \verb|cleanData(rawData: RawData) → CleanData| \\
    \textbf{Function:} Removes noise and corrects errors in the structured data.
    
    \item \textbf{DataPreprocessing} \\
    \textbf{Method:} \verb|normalizeData(cleanData: CleanData) → NormalizedData| \\
    \textbf{Function:} Standardizes units, formats, and scales data for consistent further processing.
\end{itemize}

\subsection{4. Feature Engineering}
\begin{itemize}
    \item \textbf{FeatureEngineering} \\
    \textbf{Method:} \verb|extractFeatures(data: NormalizedData) → FeatureVector| \\
    \textbf{Function:} Converts normalized data into a feature vector that captures essential parameters.
    
    \item \textbf{FeatureEngineering} \\
    \textbf{Method:} \verb|aggregateFeatures(features: FeatureVector) → AggregatedData| \\
    \textbf{Function:} Aggregates individual features into summary statistics or higher-level representations.
\end{itemize}

\subsection{5. Modeling}

\subsubsection{5A. Object Detection (YOLO)}
\begin{itemize}
    \item \textbf{YOLODetectionModel} \\
    \textbf{Method:} \verb|trainYOLO(features: FeatureVector) → Model| \\
    \textbf{Function:} Trains the YOLO model to detect key regions (bounding boxes) in blueprint images.
    
    \item \textbf{YOLODetectionModel} \\
    \textbf{Method:} \verb|predictYOLO(features: FeatureVector) → BoundingBoxes| \\
    \textbf{Function:} Predicts bounding boxes to identify regions of interest.
\end{itemize}

\subsubsection{5B. Symbol Classification}
\begin{itemize}
    \item \textbf{SymbolClassificationModel} \\
    \textbf{Method:} \verb|trainSymbolClassifier(croppedImage: Image) → Model| \\
    \textbf{Function:} Trains a model to classify symbols in cropped image regions.
    
    \item \textbf{SymbolClassificationModel} \\
    \textbf{Method:} \verb|predictSymbols(croppedImage: Image) → Prediction| \\
    \textbf{Function:} Classifies symbols (e.g., arrows, icons) based on the cropped input.
\end{itemize}

\subsubsection{5C. OCR for Text Extraction}
\begin{itemize}
    \item \textbf{OCRModel} \\
    \textbf{Method:} \verb|trainOCR(imageData: ImageData) → Model| \\
    \textbf{Function:} Trains an OCR model specifically tuned to your blueprint data.
    
    \item \textbf{OCRModel} \\
    \textbf{Method:} \verb|extractText(image: Image) → TextData| \\
    \textbf{Function:} Extracts precise textual information from cropped regions.
\end{itemize}

\subsubsection{5D. Rule-Based Feasibility Analysis}
\begin{itemize}
    \item \textbf{RuleBasedFeasibilityModel} \\
    \textbf{Method:} \verb|applyRules(yoloOutput: BoundingBoxes, symbolOutput: Prediction, ocrOutput: TextData) → FeasibilityResult| \\
    \textbf{Function:} Integrates outputs from YOLO, symbol classification, and OCR models to determine if production tolerances are feasible.
\end{itemize}

\subsection{6. Deployment and Monitoring}
\begin{itemize}
    \item \textbf{ModelDeployment} \\
    \textbf{Method:} \verb|packageModel(model: Model) → DeploymentPackage| \\
    \textbf{Function:} Packages the final feasibility model for deployment.
    
    \item \textbf{ModelDeployment} \\
    \textbf{Method:} \verb|deployModel(deploymentPackage: DeploymentPackage) → Endpoint| \\
    \textbf{Function:} Deploys the model (e.g., as a REST API) for real-time usage.
    
    \item \textbf{Monitoring} \\
    \textbf{Method:} \verb|monitorPerformance(endpoint: Endpoint) → PerformanceMetrics| \\
    \textbf{Function:} Continuously monitors the model's performance in production.
    
    \item \textbf{Monitoring} \\
    \textbf{Method:} \verb|triggerRetraining() → None| \\
    \textbf{Function:} Initiates retraining if performance degrades.
\end{itemize}

\subsection{7. Pipeline Orchestration}
\begin{itemize}
    \item \textbf{PipelineManager} \\
    \textbf{Method:} \verb|runPipeline() → None| \\
    \textbf{Function:} Orchestrates the entire workflow---from data ingestion, extraction, preprocessing, feature engineering, and modeling, to deployment and monitoring---ensuring each step is executed in the correct sequence.
\end{itemize}

\section{Why the Steps Are Organized This Way}
\begin{itemize}
    \item \textbf{Entry Point and Standardization:} \\
    Data ingestion and file parsing are the entry points, ensuring all raw data is collected and converted to a consistent format.
    
    \item \textbf{Specialized Extraction:} \\
    The blueprint extraction step converts visual, unstructured data into meaningful structured data. This is critical because subsequent processing relies on having accurate, domain-specific data.
    
    \item \textbf{Cleaning and Normalization:} \\
    Data preprocessing refines the extracted data by cleaning and normalizing it, so that the feature engineering step can effectively generate numerical representations.
    
    \item \textbf{Feature Engineering for ML Models:} \\
    Feature engineering transforms cleaned data into input vectors, which are necessary for training the object detection, symbol classification, and OCR models.
    
    \item \textbf{Modeling Sequence:} \\
    \begin{itemize}[label={--}]
      \item \textbf{YOLODetectionModel} runs first to locate areas of interest.
      \item Outputs from YOLO then drive the symbol classification and OCR tasks, ensuring that only relevant image areas are analyzed.
      \item The rule-based feasibility step integrates the outputs to make the final production feasibility determination.
    \end{itemize}
    
    \item \textbf{Deployment and Feedback:} \\
    Once the decision is made, the model is packaged and deployed, and its performance is continuously monitored to ensure long-term reliability.
    
    \item \textbf{Orchestration:} \\
    The PipelineManager oversees the entire workflow, ensuring each dependency is met and steps are executed in the correct order, maintaining the integrity and efficiency of the pipeline.
\end{itemize}

\section{Conclusion}
This ML pipeline is a comprehensive, modular, and sequential process designed to address the complexities of extracting and analyzing blueprint data for production feasibility. The modular design allows individual components to be developed, tested, and maintained independently, while the orchestration layer ensures smooth integration of all parts. This document, along with the accompanying UML diagram, provides a clear roadmap for both implementation and future development.

\end{document}
